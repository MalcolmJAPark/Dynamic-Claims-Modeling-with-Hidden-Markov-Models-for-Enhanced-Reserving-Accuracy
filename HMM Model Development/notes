First process is to decide time-step granularity: although monthly would be a common period in reserving work, but to smooth out the noise
in claim counts data, use quarterly.
To do so, we will aggregate the cleaned data into a time series, s.t.
* frequency series: for each period t, count number of unique id's whose dateOfLoss falls in period
* severity series: for each period t, compute a summary of severities 
(S_t = 1/N_t \sum_{i:t_i=t} log(netBuildingPaymentAmount_i + netContentsPaymentAmount_i) where N_t is period's claim count.
* exposure: record policyCount (normalized exposures) in each period.

aggregate_quarterly.py runs with combined_data_clean.csv and produces aggregated_quarterly.csv with columns:
quarter (e.g. “2025Q1”)
n_claims (frequency)
avg_log_severity (severity)
policyCount (exposure)
quarter_start (timestamp for plot axes)

To run: python3 aggregate_quarterly.py
