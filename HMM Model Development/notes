First process is to decide time-step granularity: although monthly would be a common period in reserving work, but to smooth out the noise
in claim counts data, use quarterly.
To do so, we will aggregate the cleaned data into a time series, s.t.
* frequency series: for each period t, count number of unique id's whose dateOfLoss falls in period
* severity series: for each period t, compute a summary of severities 
(S_t = 1/N_t \sum_{i:t_i=t} log(netBuildingPaymentAmount_i + netContentsPaymentAmount_i) where N_t is period's claim count.
* exposure: record policyCount (normalized exposures) in each period.

aggregate_quarterly.py runs with combined_data_clean.csv and produces aggregated_quarterly.csv with columns:
quarter (e.g. “2025Q1”)
n_claims (frequency)
avg_log_severity (severity)
policyCount (exposure)
quarter_start (timestamp for plot axes)

To run: python3 aggregate_quarterly.py


Second process is to create visualizations:
plot n_t and S_t over time and check for outliers, trends, and seasonality. (computing summary statistics (mean, var) of n_t and log(severity))
this will sanity-check aggregation and guide initialization.

analyze_time_series.py runs with aggregated_quarterly.csv which compute and prints means and variances of n_claims and avg_log_severity, 
detect outliers (|z|>3) and print out the quarter-periods, and produces: 
Quarterly counts with a 4-quarter moving average and outliers (n_claims_time_series.png)
Quarterly avg log severity with trend and outliers (avg_log_severity_time_series.png)
Seasonality bar charts for counts (n_claims_seasonality.png)
Seasonality bar charts for severity (avg_log_severity_seasonality.png)

Third process is to prototype a 2-state HMM model on frequency (n_claims) only.
We will be using hmmlearn's PoissonHMM (2 components) to fit {n_t} --> https://hmmlearn.readthedocs.io/en/latest/ and inspect learned 
λ_low and λ_high and transition matrix. Then, plot log-likelihood vs. iteration to confirm convergence.

fit_poisson_hmm.py runs with aggregated_quarterly.csv and fits a 2 state Poiss HMM to {n_t}.
Then prints out the estimated Poiss means (λ_0, λ_1), trans matrix, and initial state of probabilities. Additionally saves a convergence 
plot (hmm_log_likelihood.png) showing log-likelihood by EM iteration.

Fourth process (only if third process works): to gain a truly joint model, roll out custom EM to specfiy 
mixed emission distributions (Poisson + LogNormal).

joint_hmm_em.py runs with aggregated_quarterly.csv and 
* converts avg_log_severity back to severity via exp
* intitalizes a 2-state HMM with:
  * poiss emissions for counts (λ_k)
  * Log-Normal emissions for severities (μ_k, σ_k)
* Runs Baum-Welch EM loop (forward-backward + parameter updates)
* Prints the final π, A, λ_k, μ_k, and σ_k
* Saves a log-likelihood convergence plot (joint_hmm_em_loglikelihood.png)

Fifth process is to decode states with Viterbi algorithm and validate that "high-risk" periods corresponds to spikes in counts/severity.
To validate transition probabilities (e.g., P(low --> high) is relatively small).
For visual confirmation of regime assignments, overlay on time series.

decode_and_plot.py runs with aggregated_quarterly.csv and
* runs custom em to estimate π, A, λ_k, μ_k, and σ_k
* decodes the most likely state sequence via Viterbi
* overlays the decoded regimes on two time-series (counts and avg log-severity)
