Logbook:
First, compute the posterior at asOfDate.
* Run forward backward algorithm on historical claim counts and severities to get filtered probability of being in each latent state at the 
end of observation window.
* This gives P(S_T = l | data_{1:T}) for l ∈ {low-risk, high-risk}

posterior_at_asofdate.py loads aggregated_quarterly.csv and intializes HMM parameters.
Then, it runs forward-backward algorithm to compute the full smoothing posterior γ_t,k = P(S_t = k | data).
Finally, it will print the filtered probabilities at the final period T:
* P(S_T = low | data)
* P(S_T = high | data)

Bug: prints a posterior using initialized (unfitted) parameters unless manually plug in EM-fitted
Fix: make s.t. accept fitted params and call forward-backward once.

The next step is to simulate future latent-state trajectories since counts, severities, and reserves depends (hangs) off these params.
Concretely:
1. simulation parameters: N number of Monte Carlo paths (e.g., 5000 - 10000 to get stable percentiles
2. simulation parameters: H how many quarters beyong cut-off T we need to project
3. Seed each path at T:
'''
# π_T is your 2×1 vector of filtered P(state=T | data)
# A is your 2×2 transition matrix
import numpy as np

def sample_initial_states(pi_T, N):
    # returns an array of length N with 0=low-risk, 1=high-risk
    return np.random.choice([0,1], size=N, p=pi_T)
'''
4. Step Markov chain forward:
'''
def simulate_state_paths(A, initial_states, H):
    N = len(initial_states)
    states = np.zeros((N, H+1), dtype=int)
    states[:,0] = initial_states

    for h in range(1, H+1):
        # for each path, sample next state from A[row = last state]
        probs = A[states[:,h-1]]        # shape (N, 2)
        # vectorized draw: for each path i, choose 0/1 by probs[i]
        u = np.random.rand(N)            # uniform(0,1) for each path
        # if u < P(st=1), pick 1, else 0
        states[:,h] = (u < probs[:,1]).astype(int)
    return states   # shape (N, H+1)
'''
4. sanity check: Simulate a small batch (e.g., N = 1000, H = 4) and tabulate empirical transition freq vs. A to ensure sampler is correct.

simulate_state_paths.py - replace pi_T and A with fitted values


Next step is to integrate our state-conditional frequency and severity draws s.t. for each simulated path and quarter, we will:
* draw a claim count: N_{i,h} ~ CountDist(λ_{s_{i,h}}) where s_{i,h} ∈ {0,1} is the latent state of path i in quarter T+h, and λ_0, λ_1 are 
our poiss (NB) rates for low and high risk
* draw severities for each claim: X^j_{i,h} ~ SeverityDist(θ_{s_{i,h}}), j=1,..., N_{i,h}, e.g.,
    - Log-Normal(μ_0, σ_0), vs. (μ_1, σ_1), or
    - Gamma(α_0, β_0), vs (α_1, β_1)
* aggregate to get total dollars: C_{i,h} = sum^{N_{i,h}}_j=1 X^j_{i,h}.
* store {N_{i,h}, C_{i,h}} for all i=1,..., N and h=1,..., H

simulate_frequency_severity.py seeds initial states from filtered posterior at the cut-off and simulates the Markov chain forward H quarters. Then, it draws claim counts via Poisson and 
severities via Log-Normal and aggregates into a DataFrame with columns (path, horizon, state, n_claims, total_loss).
Then, it returns simulated-claims.csv and full Monte Carlo sample of future counts and losses.


Next step is to translate the following sample cash-flows into IBNR reserves for each simulation path. Essentially, this process can be broken down into two steps:
1. Allocate each simulated claim to "reported" vs. "unreported". Need claim-reporting delay model - e.g., an empirical delay distribution G(d) giving the probability a claim is reported 
d quarters after occurence. Then for each individual claim in simulation:
    * draw a reporting lag L^(j)_{i,h} ~ G(d) for the jth claim in path i, quarter T+h
    * determine if it's IBNR at the valuation date, meaning any claim that occurs in quarter T+h and is drawn to report at T+h+L is still unreported as of cut-off T if T+h+L > T --> h+L>0.
    So we flag it as IBNR whenever L^(j)_{i,h} > -h.
    * Sum the "late" severities, meaning for each path i we define IBNR_i = sum^H_{h=1} sum^{N_{i,h}}_j=1 1{L^(j)_{i,h} > -h} X^(j)_{i,h}.
2. Discount and aggregate to get a reserve distribution. Once we have {IBNR}^N_i=1:
    * present-value discounting (if required)
    * Empirical summaries, meaning compute Quantile_a = the ath percentile of {IBNR_i}

simulate_ibnr_reserves.py:
1. Simulate initial latent states at the cut‐off and future trajectories.
2. Draw claim counts and severities per state and quarter.
3. Sample reporting delays from your empirical delay distribution G(d).
4. Flag each claim as IBNR if L > -h (i.e. still unreported by cut‐off when it occurred at horizon h).
5. Aggregate raw and present‐value reserves per Monte Carlo path.
6. Compute percentile summaries of the reserve distribution.
7. Save the detailed path‐level results to 'simulated_reserves.csv'.

simulate_ibnr_reserves.py is really forward "next-H" ultimate: we simulate future occurrences and treat all of them as "IBNR at T". While useful for planning not strict IBNR.
The script strict_ibnr_at_T.py computes strict IBNR at the valuation date T: only claims already incurred by T but not yet reported (based on the lag tail P(L>T−t)).

Bug: IBNR flag 1{L > -h} makes every future claim count as "IBNR at T", which is not strict IBNR.
Bug: because historical realized counts N_t are known, the number still unreported at T should be Binomial(N_t, p_tail(T-t)), not poisson.
