Logbook:
First, compute the posterior at asOfDate.
* Run forward backward algorithm on historical claim counts and severities to get filtered probability of being in each latent state at the 
end of observation window.
* This gives P(S_T = l | data_{1:T}) for l ∈ {low-risk, high-risk}

posterior_at_asofdate.py loads aggregated_quarterly.csv and intializes HMM parameters.
Then, it runs forward-backward algorithm to compute the full smoothing posterior γ_t,k = P(S_t = k | data).
Finally, it will print the filtered probabilities at the final period T:
* P(S_T = low | data)
* P(S_T = high | data)

The next step is to simulate future latent-state trajectories since counts, severities, and reserves depends (hangs) off these params.
Concretely:
1. simulation parameters: N number of Monte Carlo paths (e.g., 5000 - 10000 to get stable percentiles
2. simulation parameters: H how many quarters beyong cut-off T we need to project
3. Seed each path at T:
'''
# π_T is your 2×1 vector of filtered P(state=T | data)
# A is your 2×2 transition matrix
import numpy as np

def sample_initial_states(pi_T, N):
    # returns an array of length N with 0=low-risk, 1=high-risk
    return np.random.choice([0,1], size=N, p=pi_T)
'''
4. Step Markov chain forward:
'''
def simulate_state_paths(A, initial_states, H):
    N = len(initial_states)
    states = np.zeros((N, H+1), dtype=int)
    states[:,0] = initial_states

    for h in range(1, H+1):
        # for each path, sample next state from A[row = last state]
        probs = A[states[:,h-1]]        # shape (N, 2)
        # vectorized draw: for each path i, choose 0/1 by probs[i]
        u = np.random.rand(N)            # uniform(0,1) for each path
        # if u < P(st=1), pick 1, else 0
        states[:,h] = (u < probs[:,1]).astype(int)
    return states   # shape (N, H+1)
'''
4. sanity check: Simulate a small batch (e.g., N = 1000, H = 4) and tabulate empirical transition freq vs. A to ensure sampler is correct.


Next step is to integrate our state-conditional frequency and severity draws s.t. for each simulated path and quarter, we will:
* draw a claim count: N_{i,h} ~ CountDist(λ_{s_{i,h}}) where s_{i,h} ∈ {0,1} is the latent state of path i in quarter T+h, and λ_0, λ_1 are 
our poiss (NB) rates for low and high risk
* draw severities for each claim: X^j_{i,h} ~ SeverityDist(θ_{s_{i,h}}), j=1,..., N_{i,h}, e.g.,
    - Log-Normal(μ_0, σ_0), vs. (μ_1, σ_1), or
    - Gamma(α_0, β_0), vs (α_1, β_1)
* aggregate to get total dollars: C_{i,h} = sum^{N_{i,h}}_j=1 X^j_{i,h}.
* store {N_{i,h}, C_{i,h}} for all i=1,..., N and h=1,..., H



