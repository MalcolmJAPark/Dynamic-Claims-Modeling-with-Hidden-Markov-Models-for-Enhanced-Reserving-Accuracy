Logbook:
First, compute the posterior at asOfDate.
* Run forward backward algorithm on historical claim counts and severities to get filtered probability of being in each latent state at the 
end of observation window.
* This gives P(S_T = l | data_{1:T}) for l ∈ {low-risk, high-risk}

posterior_at_asofdate.py loads aggregated_quarterly.csv and intializes HMM parameters.
Then, it runs forward-backward algorithm to compute the full smoothing posterior γ_t,k = P(S_t = k | data).
Finally, it will print the filtered probabilities at the final period T:
* P(S_T = low | data)
* P(S_T = high | data)

The next step is to simulate future latent-state trajectories since counts, severities, and reserves depends (hangs) off these params.
Concretely:
1. simulation parameters: N number of Monte Carlo paths (e.g., 5000 - 10000 to get stable percentiles
2. simulation parameters: H how many quarters beyong cut-off T we need to project
3. Seed each path at T:
'''
# π_T is your 2×1 vector of filtered P(state=T | data)
# A is your 2×2 transition matrix
import numpy as np

def sample_initial_states(pi_T, N):
    # returns an array of length N with 0=low-risk, 1=high-risk
    return np.random.choice([0,1], size=N, p=pi_T)
'''
4. Step Markov chain forward:
'''
def simulate_state_paths(A, initial_states, H):
    N = len(initial_states)
    states = np.zeros((N, H+1), dtype=int)
    states[:,0] = initial_states

    for h in range(1, H+1):
        # for each path, sample next state from A[row = last state]
        probs = A[states[:,h-1]]        # shape (N, 2)
        # vectorized draw: for each path i, choose 0/1 by probs[i]
        u = np.random.rand(N)            # uniform(0,1) for each path
        # if u < P(st=1), pick 1, else 0
        states[:,h] = (u < probs[:,1]).astype(int)
    return states   # shape (N, H+1)
'''
4. sanity check: Simulate a small batch (e.g., N = 1000, H = 4) and tabulate empirical transition freq vs. A to ensure sampler is correct.

